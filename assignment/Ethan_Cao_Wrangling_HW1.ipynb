{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuneWayne/wrangling/blob/main/assignment/Ethan_Cao_Wrangling_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling\n",
        "## `! git clone https://github.com/DS3001/wrangling`\n",
        "## Do Q2, and one of Q1 or Q3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "- <font color=\"lightgreen\">This paper is about researching for an effective and easy way to conduct data cleaning. To create a framework of data cleaning that makes all datasets easy to work with.</font>\n",
        "\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "- <font color=\"lightgreen\">The 'tidy data standard' is intended to serve the purpose of making data cleaning more efficient, such as reducing redundant efforts and simplifying initial exploration when data is converted to a rather predicatble structure. It is also to enhance the compatibility between data analysis tools with the cleaned data set, as it eliminates the need for constant data reshaping. It also makes it easy for different software to communicate seamlessly. This overall allows analysts to focus on insights rather than the logistics of resolving technical details in reshaping and restructuring. </font>\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "- <font color=\"lightgreen\">The first sentence is likely referring to the idea that Tidy datasets follow a consistent structure, while messy datasets can be disorganized in countless different ways, there's no standardized formula on how messy a dataset can be. The second sentence is referring to the idea that when looking at a specific dataset, it's usually clear what counts as a variable and what counts as an observation. But it is tricky to define them across all disciplines with a generalized definition. Such as one row in a medical dataset represents a patient, but in a time series dataset, each row might represent a daily measurement for one patient.</font>\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "- <font color=\"lightgreen\">Values are defined as a collection of values in a dataset, which could be quantitative numberes or qualitative strings. such as heigh, weight vs names, categories. Variables are defined as something that contains values that measure the same attribute, such as temperature, where each column in a dataset is represented as one variable. Observations are defined as all values measured on the same unit across multiple features, which can be interpreted as the rows in a dataset.</font>\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "- <font color=\"lightgreen\">Tidy data is defined as a standardized way of structuring datasets, specifically, each variable forms a column, each observation forms a row, and each type of observational unit forms a table </font>\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "- <font color=\"lightgreen\">1. Column headers are values, not variable names, instead of having meaningful variable names, people chose data values to represent column headers. 2. Multiple variables are stored in one column, such as Height_Weight stored together instead of separate columns. 3. Variables are stored in both rows and columns, such as a dataset measuring different months appear as column headers instead of values in a Month column. 4. Multiple types of observational units are stored in the same table, where one table mixes different types of data, which should be split into separate tables. Table 4 is messy becuase its column headers are values, not variable names, it doesn't follow the tidy data structure, with incoe elvels being spread across multiple columns. Its also difficult to extract variables as to get all data related to income, one must manually select multiple columns instead of just one. Melting a dataset is the process of transforming wide-format data into a long-format structure by turning column headers into values.  </font>\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "- <font color=\"lightgreen\">Table 11 is messy because column headers are values, not variable names. Variables are stored in both rows and columns, and there are many missing (null) values in the dataset. Table 12 is molten because the day columns have been melted into a date column. Moreover, the element column has been spread into two separate columns: tmax and tmin, and each row now represents a single day's observation, which follows the tidy data standard.</font>\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?\n",
        "- <font color=\"lightgreen\">the author describes a chicken-and-egg problem in relation to tidy data because titdy data is only useful if there are tools that work well with it. Such as ggplot2, dplyr, and tidyverse. If analysts are alrady using messy data formats, developing new tidy tools won't help unless people adopt tidy data structures. The author's hopes for the future of data wrangling is that he hopes that there will be incremental improvements in tidy tools and data structures. Also he hopes that there will be alternative formulations of tidiness, such as some datasets will be using multidimensional arrays rather than tables, followed by tools that can support both array-tidy and dataframe tidy formats. Next, he hopes that using user-testing and cognitive research to design better tools for data wrangling. Moreover, he wish to expand beyond tidying to other data cleaning challenges, such as having new frameworks to be developed to standardize and simplify the tasks of data cleaning.  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.\n",
        "import pandas as pd\n",
        "df = pd.read_csv('airbnb_hw.csv')\n",
        "# check for unique values in price\n",
        "df['Price'].unique()\n",
        "# check for null values, apparantly there are none\n",
        "df['Price'].isnull().sum()\n",
        "df['Price'].isnull().value_counts()\n",
        "# check for nans, it returns a false\n",
        "df['Price'].hasnans\n",
        "# check for data type of price\n",
        "df['Price'].dtypes\n",
        "# it seems like the price column is an object, remove any potential spaces or symbols that might have made it a string\n",
        "df['Price'] = df['Price'].str.replace('$', '')\n",
        "df['Price'] = df['Price'].str.replace(',', '')\n",
        "df['Price'] = df['Price'].str.replace(' ', '')\n",
        "df['Price'] = df['Price'].astype(float)\n",
        "# check type of price again, now it should be float\n",
        "df['Price'].dtypes\n",
        "df['Price'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7MrIerWZNgT",
        "outputId": "32035f73-5df7-46a3-8605-b1053dcf3193"
      },
      "id": "q7MrIerWZNgT",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "df2 = pd.read_csv('sharks.csv')\n",
        "# check for unique values in type, there's a null value\n",
        "df2['Type'].unique()\n",
        "# check for null values, there are null values in this case, there are five nan values in total\n",
        "df2['Type'].hasnans\n",
        "df2['Type'].isna().sum() # 5 null values\n",
        "# drop the null values\n",
        "df2 = df2.dropna(subset=['Type'])\n",
        "# It seems like there are overlapping unique variables\n",
        "df2['Type'].value_counts()\n",
        "# merge overlapping variables into the same categories\n",
        "type_replacement = {\n",
        "    'Boat':'Watercraft',\n",
        "    'Boatomg':'Watercraft',\n",
        "    'Boating':'Watercraft',\n",
        "    'Sea Disaster':'Watercraft',\n",
        "    'Questionable': np.nan,\n",
        "    'Unconfirmed': np.nan,\n",
        "    'Unverified': np.nan,\n",
        "    'Under investigation': np.nan,\n",
        "    'Invalid': np.nan\n",
        "}\n",
        "\n",
        "df2['Type'] = df2['Type'].replace(type_replacement)\n",
        "df2['Type'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "dyCuKbHN3FXN",
        "outputId": "194fc3e9-08ae-4b89-dffb-5b923a08e9b1"
      },
      "id": "dyCuKbHN3FXN",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-74-62e5ba8eefc9>:4: DtypeWarning: Columns (10,17,18,19,20,21,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df2 = pd.read_csv('sharks.csv')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Type\n",
              "Unprovoked    4716\n",
              "Provoked       593\n",
              "Watercraft     583\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Type</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Unprovoked</th>\n",
              "      <td>4716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Provoked</th>\n",
              "      <td>593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Watercraft</th>\n",
              "      <td>583</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.\n",
        "df3 = pd.read_parquet('justice_data.parquet')\n",
        "# check unique values\n",
        "print(df3['WhetherDefendantWasReleasedPretrial'].value_counts())\n",
        "# there's a third variable 9 which represents unclear conclusions, we'll treat them as null\n",
        "df3['WhetherDefendantWasReleasedPretrial'].unique()\n",
        "df3['WhetherDefendantWasReleasedPretrial'] = df3['WhetherDefendantWasReleasedPretrial'].replace(9, np.nan)\n",
        "print(df3['WhetherDefendantWasReleasedPretrial'].value_counts())\n",
        "# there are now 31 missing values in the dataset\n",
        "print(df3['WhetherDefendantWasReleasedPretrial'].isna().sum())\n",
        "df3['WhetherDefendantWasReleasedPretrial'].hasnans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZE0_CGbl-Kc-",
        "outputId": "9414613b-e160-4aac-c596-282e0e3cac6f"
      },
      "id": "ZE0_CGbl-Kc-",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WhetherDefendantWasReleasedPretrial\n",
            "1    19154\n",
            "0     3801\n",
            "9       31\n",
            "Name: count, dtype: int64\n",
            "WhetherDefendantWasReleasedPretrial\n",
            "1.0    19154\n",
            "0.0     3801\n",
            "Name: count, dtype: int64\n",
            "31\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.\n",
        "Imp_Sen = df3['ImposedSentenceAllChargeInContactEvent']\n",
        "Sen_type = df3['SentenceTypeAllChargesAtConvictionInContactEvent']\n",
        "# immediately there are missing values denoted by spaces\n",
        "Imp_Sen.unique()\n",
        "# apparently its also in a string mode\n",
        "print(Imp_Sen.dtype)\n",
        "# force it to convert to numeric values\n",
        "Imp_Sen = pd.to_numeric(Imp_Sen, errors = 'coerce')\n",
        "# 9053 missing values identified\n",
        "Imp_Sen.isnull().sum()\n",
        "# check how missing values in the first column correlates with the second column\n",
        "print(pd.crosstab(Imp_Sen.isnull(), Sen_type))\n",
        "# replace dismissed charges (category 4) with sentence length of 0\n",
        "Imp_Sen = Imp_Sen.mask(Sen_type == 4, 0)\n",
        "# replace missing values with null label\n",
        "Imp_Sen = Imp_Sen.mask(Sen_type == 9, np.nan)\n",
        "# cross tab to check missing values number in category 9, only 274 null values left\n",
        "print(pd.crosstab(Imp_Sen.isnull(), Sen_type))\n",
        "df3['ImposedSentenceAllChargeInContactEvent'] = Imp_Sen\n",
        "# delete temporary variables\n",
        "del Imp_Sen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mBPMfm5MyuE",
        "outputId": "9af5edfd-19eb-4d6e-a228-473e5eb4af06"
      },
      "id": "_mBPMfm5MyuE",
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float64\n",
            "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
            "ImposedSentenceAllChargeInContactEvent                                      \n",
            "False                                             8720  4299  914  8779    0\n",
            "True                                                 0     0    0     0  274\n",
            "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
            "ImposedSentenceAllChargeInContactEvent                                      \n",
            "False                                             8720  4299  914  8779    0\n",
            "True                                                 0     0    0     0  274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q3.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}